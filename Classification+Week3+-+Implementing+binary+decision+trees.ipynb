{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to implement your own binary decision tree classifier. You will:\n",
    "\n",
    "Use SFrames to do some feature engineering.\n",
    "Transform categorical variables into binary variables.\n",
    "Write a function to compute the number of misclassified examples in an intermediate node.\n",
    "Write a function to find the best feature to split on.\n",
    "Build a binary decision tree from scratch.\n",
    "Make predictions using the decision tree.\n",
    "Evaluate the accuracy of the decision tree.\n",
    "Visualize the decision at the root node.\n",
    "Important Note: In this assignment, we will focus on building decision trees where the data contain only binary (0 or 1) features. This allows us to avoid dealing with:\n",
    "\n",
    "Multiple intermediate nodes in a split\n",
    "The thresholding issues of real-valued features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.18.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.19.1'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "print(numpy.version.version)\n",
    "import scipy\n",
    "scipy.version.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinwang/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2698: DtypeWarning: Columns (19,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "loans = pd.read_csv('/Users/kevinwang/Documents/Coursera/Machine Learning - Classification/Week3/lending-club-data.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>member_id</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>funded_amnt</th>\n",
       "      <th>funded_amnt_inv</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>grade</th>\n",
       "      <th>sub_grade</th>\n",
       "      <th>...</th>\n",
       "      <th>sub_grade_num</th>\n",
       "      <th>delinq_2yrs_zero</th>\n",
       "      <th>pub_rec_zero</th>\n",
       "      <th>collections_12_mths_zero</th>\n",
       "      <th>short_emp</th>\n",
       "      <th>payment_inc_ratio</th>\n",
       "      <th>final_d</th>\n",
       "      <th>last_delinq_none</th>\n",
       "      <th>last_record_none</th>\n",
       "      <th>last_major_derog_none</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1077501</td>\n",
       "      <td>1296599</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "      <td>4975</td>\n",
       "      <td>36 months</td>\n",
       "      <td>10.65</td>\n",
       "      <td>162.87</td>\n",
       "      <td>B</td>\n",
       "      <td>B2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.14350</td>\n",
       "      <td>20141201T000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1077430</td>\n",
       "      <td>1314167</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>60 months</td>\n",
       "      <td>15.27</td>\n",
       "      <td>59.83</td>\n",
       "      <td>C</td>\n",
       "      <td>C4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.39320</td>\n",
       "      <td>20161201T000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1077175</td>\n",
       "      <td>1313524</td>\n",
       "      <td>2400</td>\n",
       "      <td>2400</td>\n",
       "      <td>2400</td>\n",
       "      <td>36 months</td>\n",
       "      <td>15.96</td>\n",
       "      <td>84.33</td>\n",
       "      <td>C</td>\n",
       "      <td>C5</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.25955</td>\n",
       "      <td>20141201T000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1076863</td>\n",
       "      <td>1277178</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>36 months</td>\n",
       "      <td>13.49</td>\n",
       "      <td>339.31</td>\n",
       "      <td>C</td>\n",
       "      <td>C1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.27585</td>\n",
       "      <td>20141201T000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1075269</td>\n",
       "      <td>1311441</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "      <td>36 months</td>\n",
       "      <td>7.90</td>\n",
       "      <td>156.46</td>\n",
       "      <td>A</td>\n",
       "      <td>A4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.21533</td>\n",
       "      <td>20141201T000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  member_id  loan_amnt  funded_amnt  funded_amnt_inv        term  \\\n",
       "0  1077501    1296599       5000         5000             4975   36 months   \n",
       "1  1077430    1314167       2500         2500             2500   60 months   \n",
       "2  1077175    1313524       2400         2400             2400   36 months   \n",
       "3  1076863    1277178      10000        10000            10000   36 months   \n",
       "4  1075269    1311441       5000         5000             5000   36 months   \n",
       "\n",
       "   int_rate  installment grade sub_grade  ... sub_grade_num delinq_2yrs_zero  \\\n",
       "0     10.65       162.87     B        B2  ...           0.4              1.0   \n",
       "1     15.27        59.83     C        C4  ...           0.8              1.0   \n",
       "2     15.96        84.33     C        C5  ...           1.0              1.0   \n",
       "3     13.49       339.31     C        C1  ...           0.2              1.0   \n",
       "4      7.90       156.46     A        A4  ...           0.8              1.0   \n",
       "\n",
       "  pub_rec_zero  collections_12_mths_zero short_emp payment_inc_ratio  \\\n",
       "0          1.0                       1.0         0           8.14350   \n",
       "1          1.0                       1.0         1           2.39320   \n",
       "2          1.0                       1.0         0           8.25955   \n",
       "3          1.0                       1.0         0           8.27585   \n",
       "4          1.0                       1.0         0           5.21533   \n",
       "\n",
       "           final_d last_delinq_none last_record_none last_major_derog_none  \n",
       "0  20141201T000000                1                1                     1  \n",
       "1  20161201T000000                1                1                     1  \n",
       "2  20141201T000000                1                1                     1  \n",
       "3  20141201T000000                0                1                     1  \n",
       "4  20141201T000000                1                1                     1  \n",
       "\n",
       "[5 rows x 68 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>member_id</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>funded_amnt</th>\n",
       "      <th>funded_amnt_inv</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>dti</th>\n",
       "      <th>delinq_2yrs</th>\n",
       "      <th>...</th>\n",
       "      <th>grade_num</th>\n",
       "      <th>sub_grade_num</th>\n",
       "      <th>delinq_2yrs_zero</th>\n",
       "      <th>pub_rec_zero</th>\n",
       "      <th>collections_12_mths_zero</th>\n",
       "      <th>short_emp</th>\n",
       "      <th>payment_inc_ratio</th>\n",
       "      <th>last_delinq_none</th>\n",
       "      <th>last_record_none</th>\n",
       "      <th>last_major_derog_none</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.226070e+05</td>\n",
       "      <td>1.226070e+05</td>\n",
       "      <td>122607.000000</td>\n",
       "      <td>122607.000000</td>\n",
       "      <td>122607.000000</td>\n",
       "      <td>122607.000000</td>\n",
       "      <td>122607.000000</td>\n",
       "      <td>1.226030e+05</td>\n",
       "      <td>122607.000000</td>\n",
       "      <td>122578.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>122607.000000</td>\n",
       "      <td>122607.000000</td>\n",
       "      <td>122578.000000</td>\n",
       "      <td>122578.000000</td>\n",
       "      <td>122462.000000</td>\n",
       "      <td>122607.000000</td>\n",
       "      <td>122603.000000</td>\n",
       "      <td>122607.000000</td>\n",
       "      <td>122607.000000</td>\n",
       "      <td>122607.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.728452e+06</td>\n",
       "      <td>5.493222e+06</td>\n",
       "      <td>12809.733743</td>\n",
       "      <td>12736.123753</td>\n",
       "      <td>12497.828395</td>\n",
       "      <td>13.639487</td>\n",
       "      <td>396.623285</td>\n",
       "      <td>7.138502e+04</td>\n",
       "      <td>15.496888</td>\n",
       "      <td>0.211996</td>\n",
       "      <td>...</td>\n",
       "      <td>4.232882</td>\n",
       "      <td>0.597509</td>\n",
       "      <td>0.858107</td>\n",
       "      <td>0.908173</td>\n",
       "      <td>0.996734</td>\n",
       "      <td>0.123672</td>\n",
       "      <td>7.564725</td>\n",
       "      <td>0.588115</td>\n",
       "      <td>0.897795</td>\n",
       "      <td>0.873906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.938517e+06</td>\n",
       "      <td>6.604693e+06</td>\n",
       "      <td>7932.313398</td>\n",
       "      <td>7887.167118</td>\n",
       "      <td>7946.731527</td>\n",
       "      <td>4.390836</td>\n",
       "      <td>239.475936</td>\n",
       "      <td>5.841483e+04</td>\n",
       "      <td>7.497442</td>\n",
       "      <td>0.662052</td>\n",
       "      <td>...</td>\n",
       "      <td>1.362138</td>\n",
       "      <td>0.278934</td>\n",
       "      <td>0.348942</td>\n",
       "      <td>0.288783</td>\n",
       "      <td>0.057059</td>\n",
       "      <td>0.329208</td>\n",
       "      <td>4.127291</td>\n",
       "      <td>0.492177</td>\n",
       "      <td>0.302918</td>\n",
       "      <td>0.331957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.473400e+04</td>\n",
       "      <td>7.047300e+04</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.420000</td>\n",
       "      <td>15.670000</td>\n",
       "      <td>1.896000e+03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.532925e+05</td>\n",
       "      <td>1.064872e+06</td>\n",
       "      <td>6700.000000</td>\n",
       "      <td>6625.000000</td>\n",
       "      <td>6271.000000</td>\n",
       "      <td>10.620000</td>\n",
       "      <td>215.985000</td>\n",
       "      <td>4.405250e+04</td>\n",
       "      <td>9.880000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.362575</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.621401e+06</td>\n",
       "      <td>1.879659e+06</td>\n",
       "      <td>11000.000000</td>\n",
       "      <td>10950.000000</td>\n",
       "      <td>10500.000000</td>\n",
       "      <td>13.480000</td>\n",
       "      <td>348.180000</td>\n",
       "      <td>6.000000e+04</td>\n",
       "      <td>15.260000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.965760</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.699942e+06</td>\n",
       "      <td>8.117948e+06</td>\n",
       "      <td>17600.000000</td>\n",
       "      <td>17425.000000</td>\n",
       "      <td>17000.000000</td>\n",
       "      <td>16.290000</td>\n",
       "      <td>521.930000</td>\n",
       "      <td>8.500000e+04</td>\n",
       "      <td>20.850000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.215850</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.784128e+07</td>\n",
       "      <td>4.060424e+07</td>\n",
       "      <td>35000.000000</td>\n",
       "      <td>35000.000000</td>\n",
       "      <td>35000.000000</td>\n",
       "      <td>26.060000</td>\n",
       "      <td>1408.130000</td>\n",
       "      <td>7.141778e+06</td>\n",
       "      <td>39.880000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>54.171000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     member_id      loan_amnt    funded_amnt  \\\n",
       "count  1.226070e+05  1.226070e+05  122607.000000  122607.000000   \n",
       "mean   4.728452e+06  5.493222e+06   12809.733743   12736.123753   \n",
       "std    5.938517e+06  6.604693e+06    7932.313398    7887.167118   \n",
       "min    5.473400e+04  7.047300e+04     500.000000     500.000000   \n",
       "25%    8.532925e+05  1.064872e+06    6700.000000    6625.000000   \n",
       "50%    1.621401e+06  1.879659e+06   11000.000000   10950.000000   \n",
       "75%    6.699942e+06  8.117948e+06   17600.000000   17425.000000   \n",
       "max    3.784128e+07  4.060424e+07   35000.000000   35000.000000   \n",
       "\n",
       "       funded_amnt_inv       int_rate    installment    annual_inc  \\\n",
       "count    122607.000000  122607.000000  122607.000000  1.226030e+05   \n",
       "mean      12497.828395      13.639487     396.623285  7.138502e+04   \n",
       "std        7946.731527       4.390836     239.475936  5.841483e+04   \n",
       "min           0.000000       5.420000      15.670000  1.896000e+03   \n",
       "25%        6271.000000      10.620000     215.985000  4.405250e+04   \n",
       "50%       10500.000000      13.480000     348.180000  6.000000e+04   \n",
       "75%       17000.000000      16.290000     521.930000  8.500000e+04   \n",
       "max       35000.000000      26.060000    1408.130000  7.141778e+06   \n",
       "\n",
       "                 dti    delinq_2yrs  ...      grade_num  sub_grade_num  \\\n",
       "count  122607.000000  122578.000000  ...  122607.000000  122607.000000   \n",
       "mean       15.496888       0.211996  ...       4.232882       0.597509   \n",
       "std         7.497442       0.662052  ...       1.362138       0.278934   \n",
       "min         0.000000       0.000000  ...       0.000000       0.200000   \n",
       "25%         9.880000       0.000000  ...       3.000000       0.400000   \n",
       "50%        15.260000       0.000000  ...       4.000000       0.600000   \n",
       "75%        20.850000       0.000000  ...       5.000000       0.800000   \n",
       "max        39.880000      29.000000  ...       6.000000       1.000000   \n",
       "\n",
       "       delinq_2yrs_zero   pub_rec_zero  collections_12_mths_zero  \\\n",
       "count     122578.000000  122578.000000             122462.000000   \n",
       "mean           0.858107       0.908173                  0.996734   \n",
       "std            0.348942       0.288783                  0.057059   \n",
       "min            0.000000       0.000000                  0.000000   \n",
       "25%            1.000000       1.000000                  1.000000   \n",
       "50%            1.000000       1.000000                  1.000000   \n",
       "75%            1.000000       1.000000                  1.000000   \n",
       "max            1.000000       1.000000                  1.000000   \n",
       "\n",
       "           short_emp  payment_inc_ratio  last_delinq_none  last_record_none  \\\n",
       "count  122607.000000      122603.000000     122607.000000     122607.000000   \n",
       "mean        0.123672           7.564725          0.588115          0.897795   \n",
       "std         0.329208           4.127291          0.492177          0.302918   \n",
       "min         0.000000           0.028895          0.000000          0.000000   \n",
       "25%         0.000000           4.362575          0.000000          1.000000   \n",
       "50%         0.000000           6.965760          1.000000          1.000000   \n",
       "75%         0.000000          10.215850          1.000000          1.000000   \n",
       "max         1.000000          54.171000          1.000000          1.000000   \n",
       "\n",
       "       last_major_derog_none  \n",
       "count          122607.000000  \n",
       "mean                0.873906  \n",
       "std                 0.331957  \n",
       "min                 0.000000  \n",
       "25%                 1.000000  \n",
       "50%                 1.000000  \n",
       "75%                 1.000000  \n",
       "max                 1.000000  \n",
       "\n",
       "[8 rows x 45 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loans.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loans['safe_loans'] = loans['bad_loans'].apply(lambda x : +1 if x==0 else -1)\n",
    "#loans['safe_loans'] = loans['bad_loans'].map({1: 1, 0: -1})\n",
    "\n",
    "loans = loans.drop('bad_loans', axis = 1)\n",
    "#loans['safe_loans']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the previous assignment, we will only be considering these four features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = ['grade',              # grade of the loan\n",
    "            'term',               # the term of the loan\n",
    "            'home_ownership',     # home_ownership status: own, mortgage or rent\n",
    "            'emp_length',         # number of years of employment\n",
    "           ]\n",
    "target = 'safe_loans'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract these feature columns from the dataset, and discard the rest of the feature columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loans = loans[features + [target]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then follow the following steps:\n",
    "\n",
    "Apply one-hot encoding to loans. Your tool may have a function for one-hot encoding.\n",
    "Load the JSON files into the lists train_idx and test_idx.\n",
    "Perform train/validation split using train_idx and test_idx. In Pandas, for instance:\n",
    "\n",
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(loans):\n",
    "    return pd.get_dummies(loans)   # We don't need to identify Str variable, it will just transform 'object' var\n",
    "\n",
    "loans = one_hot(loans)\n",
    "#print(loans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training and validation\n",
    "\n",
    "Load the JSON files into the lists train_idx and validation_idx. Perform train/validation split using train_idx and validation_idx. In Pandas, for instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37224, 25)\n",
      "(9284, 25)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('/Users/kevinwang/Documents/Coursera/Machine Learning - Classification/Week3/module-5-assignment-1-train-idx.json', 'r') as f: # Reads the list of most frequent words\n",
    "    train_idx = json.load(f)\n",
    "with open('/Users/kevinwang/Documents/Coursera/Machine Learning - Classification/Week3/module-5-assignment-1-validation-idx.json', 'r') as f1: # Reads the list of most frequent words\n",
    "    validation_idx = json.load(f1)\n",
    "\n",
    "train_data = loans.loc[train_idx]\n",
    "validation_data = loans.loc[validation_idx]\n",
    "print(train_data.shape)        #(37224, 13)\n",
    "print(validation_data.shape)   #(9284, 13)\n",
    "train_loans_prop = sum(train_data['safe_loans'] == 1)/len(train_data)   #0.503653556845   Means data is very balanced\n",
    "valid_loans_prop = sum(validation_data['safe_loans'] == 1)/len(validation_data)   #0.496553209823   Means data is very balanced\n",
    "\n",
    "y_train = train_data['safe_loans']\n",
    "x_train = train_data.drop('safe_loans', axis=1)\n",
    "\n",
    "y_valid = validation_data['safe_loans']\n",
    "x_valid = validation_data.drop('safe_loans', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree implementation\n",
    "\n",
    "In this section, we will implement binary decision trees from scratch. There are several steps involved in building a decision tree. For that reason, we have split the entire assignment into several sections.\n",
    "\n",
    "Function to count number of mistakes while predicting majority class\n",
    "Recall from the lecture that prediction at an intermediate node works by predicting the majority class for all data points that belong to this node. Now, we will write a function that calculates the number of misclassified examples when predicting the majority class. This will be used to help determine which feature is the best to split on at a given node of the tree.\n",
    "\n",
    "Note: Keep in mind that in order to compute the number of mistakes for a majority classifier, we only need the label (y values) of the data points in the node.\n",
    "\n",
    "Steps to follow:\n",
    "\n",
    "Step 1: Calculate the number of safe loans and risky loans.\n",
    "Step 2: Since we are assuming majority class prediction, all the data points that are not in the majority class are considered mistakes.\n",
    "Step 3: Return the number of mistakes.\n",
    "\n",
    "Now, let us write the function intermediate_node_num_mistakes which computes the number of misclassified examples of an intermediate node given the set of labels (y values) of the data points contained in the node. Your code should be analogous to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def intermediate_node_num_mistakes(labels_in_node):    # it should named 'leaf', not 'node'\n",
    "    # Corner case: If labels_in_node is empty, return 0\n",
    "    if len(labels_in_node) == 0:\n",
    "        return 0    \n",
    "    # Count the number of 1's (actual safe loans)\n",
    "    num_safe = sum(labels_in_node == 1)  \n",
    "    # Count the number of -1's (actual risky loans)\n",
    "    num_risk = sum(labels_in_node == -1)                 \n",
    "    # Return the number of mistakes that the majority classifier makes for training data.\n",
    "    return min(num_safe, num_risk)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because there are several steps in this assignment, we have introduced some stopping points where you can check your code and make sure it is correct before proceeding. To test your intermediate_node_num_mistakes function, run the following code until you get a Test passed!, then you should proceed. Otherwise, you should spend some time figuring out where things went wrong. Again, remember that this code is specific to SFrame, but using your software of choice, you can construct similar tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n",
      "Test passed!\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "# Test case 1\n",
    "example_labels = np.array([-1, -1, 1, 1, 1])\n",
    "if intermediate_node_num_mistakes(example_labels) == 2:\n",
    "    print('Test passed!')\n",
    "else:\n",
    "    print('Test 1 failed... try again!')\n",
    "\n",
    "# Test case 2\n",
    "example_labels = np.array([-1, -1, 1, 1, 1, 1, 1])\n",
    "if intermediate_node_num_mistakes(example_labels) == 2:\n",
    "    print('Test passed!')\n",
    "else:\n",
    "    print('Test 2 failed... try again!')\n",
    "    \n",
    "# Test case 3\n",
    "example_labels = np.array([-1, -1, -1, -1, -1, 1, 1])\n",
    "if intermediate_node_num_mistakes(example_labels) == 2:\n",
    "    print('Test passed!')\n",
    "else:\n",
    "    print('Test 3 failed... try again!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to pick best feature to split on\n",
    "\n",
    "The function best_splitting_feature takes 3 arguments:\n",
    "\n",
    "1. The data\n",
    "2. The features to consider for splits (a list of strings of column names to consider for splits)\n",
    "3. The name of the target/label column (string)\n",
    "\n",
    "The function will loop through the list of possible features, and consider splitting on each of them. It will calculate the classification error of each split and return the feature that had the smallest classification error when split on.\n",
    "\n",
    "Recall that the classification error is defined as follows:\n",
    "\n",
    "classification error=# mistakes / # total examples\n",
    "\n",
    "\n",
    "Follow these steps to implement best_splitting_feature:\n",
    "\n",
    "Step 1: Loop over each feature in the feature list\n",
    "\n",
    "Step 2: Within the loop, split the data into two groups: one group where all of the data has feature value 0 or False (we will call this the left split), and one group where all of the data has feature value 1 or True (we will call this the right split). Make sure the left split corresponds with 0 and the right split corresponds with 1 to ensure your implementation fits with our implementation of the tree building process.\n",
    "\n",
    "Step 3: Calculate the number of misclassified examples in both groups of data and use the above formula to compute theclassification error.\n",
    "\n",
    "Step 4: If the computed error is smaller than the best error found so far, store this feature and its error.\n",
    "Note: Remember that since we are only dealing with binary features, we do not have to consider thresholds for real-valued features. This makes the implementation of this function much easier.\n",
    "\n",
    "Your code should be analogous to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def best_splitting_feature(data, features, target):\n",
    "    \n",
    "    target_values = data[target]\n",
    "    best_feature = None # Keep track of the best feature \n",
    "    best_error = 10     # Keep track of the best error so far \n",
    "    # Note: Since error is always <= 1, we should intialize it with something larger than 1.\n",
    "\n",
    "    # Convert to float to make sure error gets computed correctly.\n",
    "    num_data_points = float(len(data))  \n",
    "    \n",
    "    # Loop through each feature to consider splitting on that feature\n",
    "    for feature in features:\n",
    "        \n",
    "        # The left split will have all data points where the feature value is 0\n",
    "        left_split = data[data[feature] == 0]   # only keep rows where feature = 0\n",
    "        \n",
    "        # The right split will have all data points where the feature value is 1\n",
    "        ## YOUR CODE HERE\n",
    "        right_split =  data[data[feature] == 1]  # only keep rows where feature = 1\n",
    "            \n",
    "        # Calculate the number of misclassified examples in the left split.\n",
    "        # Remember that we implemented a function for this! (It was called intermediate_node_num_mistakes)\n",
    "        # YOUR CODE HERE\n",
    "        left_mistakes = intermediate_node_num_mistakes(left_split[target])            \n",
    "\n",
    "        # Calculate the number of misclassified examples in the right split.\n",
    "        ## YOUR CODE HERE\n",
    "        right_mistakes = intermediate_node_num_mistakes(right_split[target])            \n",
    "            \n",
    "        # Compute the classification error of this split.\n",
    "        # Error = (# of mistakes (left) + # of mistakes (right)) / (# of data points)\n",
    "        ## YOUR CODE HERE\n",
    "        error = (left_mistakes + right_mistakes) / len(data)\n",
    "\n",
    "        # If this is the best error we have found so far, store the feature as best_feature and the error as best_error\n",
    "        ## YOUR CODE HERE\n",
    "        if error < best_error:\n",
    "            best_error = error\n",
    "            best_feature = feature\n",
    "    return best_feature # Return the best feature we found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the tree\n",
    "\n",
    "With the above functions implemented correctly, we are now ready to build our decision tree. Each node in the decision tree is represented as a dictionary which contains the following keys and possible values:\n",
    "\n",
    "{ \n",
    "   'is_leaf'            : True/False.\n",
    "   'prediction'         : Prediction at the leaf node.\n",
    "   'left'               : (dictionary corresponding to the left tree).\n",
    "   'right'              : (dictionary corresponding to the right tree).\n",
    "   'splitting_feature'  : The feature that this node splits on\n",
    "}\n",
    "\n",
    "First, we will write a function that creates a leaf node given a set of target values. Your code should be analogous to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_leaf(target_values):    \n",
    "    # Create a leaf node\n",
    "    leaf = {'splitting_feature' : None,\n",
    "            'left' : None,\n",
    "            'right' : None,\n",
    "            'is_leaf': True    }   ## YOUR CODE HERE \n",
    "   \n",
    "    # Count the number of data points that are with actual value +1 and -1 in this node.\n",
    "    num_ones = len(target_values[target_values == +1])     # keep only row where target values = +1\n",
    "    num_minus_ones = len(target_values[target_values == -1])    \n",
    "\n",
    "    # For the leaf node, set the prediction to be the majority class.\n",
    "    # Store the predicted class (1 or -1) in leaf['prediction']\n",
    "    if num_ones > num_minus_ones:\n",
    "        leaf['prediction'] = +1         ## YOUR CODE HERE\n",
    "    else:\n",
    "        leaf['prediction'] = -1         ## YOUR CODE HERE        \n",
    "\n",
    "    # Return the leaf node\n",
    "    return leaf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have provided a function that learns the decision tree recursively and implements 3 stopping conditions:\n",
    "\n",
    "Stopping condition 1: All data points in a node are from the same class.\n",
    "Stopping condition 2: No more features to split on.\n",
    "\n",
    "Additional stopping condition: In addition to the above two stopping conditions covered in lecture, in this assignment we will also consider a stopping condition based on the max_depth of the tree. By not letting the tree grow too deep, we will save computational effort in the learning process.\n",
    "\n",
    "Now, we will provide a Python skeleton of the learning algorithm. Note that this code is not complete; it needs to be completed by you if you are using Python. Otherwise, your code should be analogous to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree_create(data, features, target, current_depth = 0, max_depth = 10):\n",
    "    remaining_features = features[:] # Make a copy of the features.\n",
    "    \n",
    "    target_values = data[target]\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    print(\"Subtree, depth = %s (%s data points).\" % (current_depth, len(target_values)))\n",
    "    \n",
    "\n",
    "    # Stopping condition 1\n",
    "    # (Check if there are mistakes at current node.\n",
    "    # Recall you wrote a function intermediate_node_num_mistakes to compute this.)\n",
    "    if  intermediate_node_num_mistakes(data[target]) == 0:  ## YOUR CODE HERE\n",
    "        print(\"Stopping condition 1 reached.\")    \n",
    "        # If not mistakes at current node, make current node a leaf node\n",
    "        return create_leaf(target_values)\n",
    "    \n",
    "    # Stopping condition 2 (check if there are remaining features to consider splitting on)\n",
    "    if remaining_features == 0:   ## YOUR CODE HERE\n",
    "        print(\"Stopping condition 2 reached.\")    \n",
    "        # If there are no remaining features to consider, make current node a leaf node\n",
    "        return create_leaf(target_values)    \n",
    "    \n",
    "    # Additional stopping condition (limit tree depth)\n",
    "    if current_depth >= max_depth:  ## YOUR CODE HERE\n",
    "        print(\"Reached maximum depth. Stopping for now.\")\n",
    "        # If the max tree depth has been reached, make current node a leaf node\n",
    "        return create_leaf(target_values)\n",
    "\n",
    "    # Find the best splitting feature (recall the function best_splitting_feature implemented above)\n",
    "    ## YOUR CODE HERE\n",
    "    splitting_feature = best_splitting_feature(data, features, target)\n",
    "    \n",
    "    # Split on the best feature that we found. \n",
    "    left_split = data[data[splitting_feature] == 0]    #keep rows where the value of that feature = 0\n",
    "    right_split = data[data[splitting_feature] == 1]   #keep rows where the value of that feature = 1\n",
    "    remaining_features.remove(splitting_feature)\n",
    "    print(\"Split on feature %s. (%s, %s)\" % (\\\n",
    "                      splitting_feature, len(left_split), len(right_split)))\n",
    "    \n",
    "    # Create a leaf node if the split is \"perfect\"    If all data goes to one side\n",
    "    if len(left_split) == len(data):\n",
    "        print(\"Creating leaf node.\")\n",
    "        return create_leaf(left_split[target])\n",
    "    if len(right_split) == len(data):\n",
    "        print(\"Creating leaf node.\")\n",
    "        return create_leaf(right_split[target])\n",
    "\n",
    "        \n",
    "    # Repeat (recurse) on left and right subtrees\n",
    "    left_tree = decision_tree_create(left_split, remaining_features, target, current_depth + 1, max_depth)        \n",
    "    ## YOUR CODE HERE\n",
    "    right_tree = decision_tree_create(right_split, remaining_features, target, current_depth + 1, max_depth) \n",
    "\n",
    "    return {'is_leaf'          : False, \n",
    "            'prediction'       : None,\n",
    "            'splitting_feature': splitting_feature,\n",
    "            'left'             : left_tree, \n",
    "            'right'            : right_tree}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the tree!\n",
    "\n",
    "Train a tree model on the train_data. Limit the depth to 6 (max_depth = 6) to make sure the algorithm doesn't run for too long. Call this tree my_decision_tree. Warning: The tree may take 1-2 minutes to learn.\n",
    "\n",
    "Making predictions with a decision tree\n",
    "\n",
    "As discussed in the lecture, we can make predictions from the decision tree with a simple recursive function. Write a function called classify, which takes in a learned tree and a test point x to classify. Include an option annotate that describes the prediction path when set to True. Your code should be analogous to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['grade_A', 'grade_B', 'grade_C', 'grade_D', 'grade_E', 'grade_F', 'grade_G', 'term_ 36 months', 'term_ 60 months', 'home_ownership_MORTGAGE', 'home_ownership_OTHER', 'home_ownership_OWN', 'home_ownership_RENT', 'emp_length_1 year', 'emp_length_10+ years', 'emp_length_2 years', 'emp_length_3 years', 'emp_length_4 years', 'emp_length_5 years', 'emp_length_6 years', 'emp_length_7 years', 'emp_length_8 years', 'emp_length_9 years', 'emp_length_< 1 year']\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 0 (37224 data points).\n",
      "Split on feature term_ 36 months. (9223, 28001)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (9223 data points).\n",
      "Split on feature grade_A. (9122, 101)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9122 data points).\n",
      "Split on feature grade_B. (8074, 1048)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (8074 data points).\n",
      "Split on feature grade_C. (5884, 2190)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (5884 data points).\n",
      "Split on feature grade_D. (3826, 2058)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (3826 data points).\n",
      "Split on feature grade_E. (1693, 2133)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (1693 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (2133 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (2058 data points).\n",
      "Split on feature grade_E. (2058, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (2190 data points).\n",
      "Split on feature grade_D. (2190, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (1048 data points).\n",
      "Split on feature emp_length_5 years. (969, 79)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (969 data points).\n",
      "Split on feature grade_C. (969, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (79 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (34, 45)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (34 data points).\n",
      "Split on feature grade_C. (34, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (45 data points).\n",
      "Split on feature grade_C. (45, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (101 data points).\n",
      "Split on feature emp_length_< 1 year. (90, 11)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (90 data points).\n",
      "Split on feature grade_B. (90, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (11 data points).\n",
      "Split on feature grade_B. (11, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (28001 data points).\n",
      "Split on feature grade_D. (23300, 4701)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23300 data points).\n",
      "Split on feature grade_E. (22024, 1276)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (22024 data points).\n",
      "Split on feature grade_F. (21666, 358)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (21666 data points).\n",
      "Split on feature grade_C. (14444, 7222)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (14444 data points).\n",
      "Split on feature grade_G. (14347, 97)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (14347 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (97 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (7222 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (4303, 2919)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (4303 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (2919 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (358 data points).\n",
      "Split on feature emp_length_8 years. (347, 11)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (347 data points).\n",
      "Split on feature grade_A. (347, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (11 data points).\n",
      "Split on feature home_ownership_OWN. (9, 2)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (9 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (2 data points).\n",
      "Stopping condition 1 reached.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (1276 data points).\n",
      "Split on feature grade_A. (1276, 0)\n",
      "Creating leaf node.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (4701 data points).\n",
      "Split on feature grade_A. (4701, 0)\n",
      "Creating leaf node.\n"
     ]
    }
   ],
   "source": [
    "#print(type(x_train.columns))        \n",
    "#print(type(list(x_train.columns)))\n",
    "#print(list(x_train.columns))\n",
    "features_new = list(x_train.columns)        # Since we did one-hot, so can not just use 'feature'\n",
    "#features_new = [col for col in x.columns]   # Since we did one-hot, so can not just use 'feature'\n",
    "print(features_new)\n",
    "my_decision_tree = decision_tree_create(train_data, features_new, target, current_depth = 0, max_depth = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions with a decision tree\n",
    "\n",
    "As discussed in the lecture, we can make predictions from the decision tree with a simple recursive function. Write a function called classify, which takes in a learned tree and a test point x to classify. Include an option annotate that describes the prediction path when set to True. Your code should be analogous to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(tree, x, annotate = False):\n",
    "       # if the node is a leaf node.\n",
    "    if tree['is_leaf']:\n",
    "        if annotate:\n",
    "             print(\"At leaf, predicting %s\" % tree['prediction'])\n",
    "        return tree['prediction']\n",
    "    else:\n",
    "        # split on feature.\n",
    "        split_feature_value = x[tree['splitting_feature']]      # x value of the test point for that splitting feature \n",
    "        if annotate:    \n",
    "             print(\"Split on %s = %s\" % (tree['splitting_feature'], split_feature_value))\n",
    "        if split_feature_value == 0:                        # If the value for that feature = 0, \n",
    "            return classify(tree['left'], x, annotate)     # if x = 0, go left\n",
    "        else:\n",
    "            return classify(tree['right'], x, annotate)     # if x = 1, go right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's consider the first example of the test set and see what my_decision_tree model predicts for this data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "safe_loans                -1\n",
      "grade_A                    0\n",
      "grade_B                    0\n",
      "grade_C                    0\n",
      "grade_D                    1\n",
      "grade_E                    0\n",
      "grade_F                    0\n",
      "grade_G                    0\n",
      "term_ 36 months            0\n",
      "term_ 60 months            1\n",
      "home_ownership_MORTGAGE    0\n",
      "home_ownership_OTHER       0\n",
      "home_ownership_OWN         0\n",
      "home_ownership_RENT        1\n",
      "emp_length_1 year          0\n",
      "emp_length_10+ years       0\n",
      "emp_length_2 years         1\n",
      "emp_length_3 years         0\n",
      "emp_length_4 years         0\n",
      "emp_length_5 years         0\n",
      "emp_length_6 years         0\n",
      "emp_length_7 years         0\n",
      "emp_length_8 years         0\n",
      "emp_length_9 years         0\n",
      "emp_length_< 1 year        0\n",
      "Name: 24, dtype: int64\n",
      "Predicted class: -1 \n"
     ]
    }
   ],
   "source": [
    "print(validation_data.iloc[0,:])\n",
    "type(validation_data.iloc[0,:])   #series\n",
    "print('Predicted class: %s ' % classify(my_decision_tree, validation_data.iloc[0,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add some annotations to our prediction to see what the prediction path was that lead to this predicted class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split on term_ 36 months = 0\n",
      "Split on grade_A = 0\n",
      "Split on grade_B = 0\n",
      "Split on grade_C = 0\n",
      "Split on grade_D = 1\n",
      "At leaf, predicting -1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(my_decision_tree, validation_data.iloc[0,:], annotate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quiz question: What was the feature that my_decision_tree first split on while making the prediction for test_data[0]?\n",
    "\n",
    "Answer: term36 \n",
    "\n",
    "Quiz question: What was the first feature that lead to a right split of test_data[0]?\n",
    "\n",
    "Answer: Grade_D\n",
    "\n",
    "Quiz question: What was the last feature split on before reaching a leaf node for test_data[0]?\n",
    "\n",
    "Answer: Grade_D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating your decision tree\n",
    "\n",
    "Now, we will write a function to evaluate a decision tree by computing the classification error of the tree on the given dataset. Write a function called evaluate_classification_error that takes in as input:\n",
    "\n",
    "1. tree (as described above)\n",
    "2. data (a data frame of data points)\n",
    "\n",
    "This function should return a prediction (class label) for each row in data using the decision tree. Your code should be analogous to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1        -1\n",
       "6        -1\n",
       "7        -1\n",
       "10       -1\n",
       "12        1\n",
       "         ..\n",
       "122572    1\n",
       "122575    1\n",
       "122588    1\n",
       "122599   -1\n",
       "122603   -1\n",
       "Length: 37224, dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#validation_data.apply(lambda x: classify(my_decision_tree, x), axis=1)\n",
    "train_data.apply(lambda x: classify(my_decision_tree, x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_classification_error(tree, data):\n",
    "    # Apply the classify(tree, x) to each row in your data\n",
    "    prediction = data.apply(lambda x: classify(tree, x), axis = 1)   #apply to each row of data, so need axis = 1\n",
    "    \n",
    "    # Once you've made the predictions, calculate the classification error and return it\n",
    "    ## YOUR CODE HERE\n",
    "    return sum(prediction != data[target])/len(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use this function to evaluate the classification error on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3804266064904363\n",
      "0.37774666092201636\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_classification_error(my_decision_tree, train_data))\n",
    "print(evaluate_classification_error(my_decision_tree, validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quiz Question: Rounded to 2nd decimal point, what is the classification error of my_decision_tree on the test_data?\n",
    "\n",
    "Answer: 0.38"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing out a decision stump\n",
    "\n",
    "As discussed in the lecture, we can print out a single decision stump (printing out the entire tree is left as an exercise to the curious reader). Here we provide Python code to visualize a decision stump. If you are using different software, make sure your code is analogous to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'term_ 36 months'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_decision_tree['splitting_feature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stump(tree, name = 'root'):\n",
    "    split_name = tree['splitting_feature'] # split_name is something like 'term. 36 months'\n",
    "    if split_name is None:\n",
    "        print (\"(leaf, label: %s)\" % tree['prediction'])\n",
    "        return None\n",
    "    split_feature, split_value = split_name.split('_')     #term_ 36 months is the dummy, if term = 36, then = 1\n",
    "    print ('                       %s' % name)\n",
    "    print ('         |---------------|----------------|')\n",
    "    print ('         |                                |')\n",
    "    print ('         |                                |')\n",
    "    print ('         |                                |')\n",
    "    print ('  [{0} == 0]               [{0} == 1]    '.format(split_name))\n",
    "    print ('         |                                |')\n",
    "    print ('         |                                |')\n",
    "    print ('         |                                |')\n",
    "    print ('    (%s)                         (%s)' \\\n",
    "        % (('leaf, label: ' + str(tree['left']['prediction']) if tree['left']['is_leaf'] else 'subtree'),\n",
    "           ('leaf, label: ' + str(tree['right']['prediction']) if tree['right']['is_leaf'] else 'subtree')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this function, we can print out the root of our decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       root\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [term_ 36 months == 0]               [term_ 36 months == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (subtree)                         (subtree)\n"
     ]
    }
   ],
   "source": [
    "print_stump(my_decision_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quiz Question: What is the feature that is used for the split at the root node?\n",
    "\n",
    "Answer: term is 36 or not.   term_ 36 months is the dummy, if term = 36, then = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the intermediate left subtree\n",
    "\n",
    "The tree is a recursive dictionary, so we do have access to all the nodes! We can use\n",
    "\n",
    "my_decision_tree['left'] to go left\n",
    "my_decision_tree['right'] to go right\n",
    "\n",
    "We can print out the left subtree by running the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       term_ 36 months\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [grade_A == 0]               [grade_A == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (subtree)                         (subtree)\n"
     ]
    }
   ],
   "source": [
    "print_stump(my_decision_tree['left'], my_decision_tree['splitting_feature'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can similarly print out the left subtree of the left subtree of the root by running the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       grade_A\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [grade_B == 0]               [grade_B == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (subtree)                         (subtree)\n"
     ]
    }
   ],
   "source": [
    "print_stump(my_decision_tree['left']['left'], my_decision_tree['left']['splitting_feature'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quiz question: What is the path of the first 3 feature splits considered along the left-most branch of my_decision_tree?\n",
    "\n",
    "Answer: term_ 36 months, grade_A, grade_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       term_ 36 months\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [grade_D == 0]               [grade_D == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (subtree)                         (leaf, label: -1)\n"
     ]
    }
   ],
   "source": [
    "print_stump(my_decision_tree['right'], my_decision_tree['splitting_feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(leaf, label: -1)\n"
     ]
    }
   ],
   "source": [
    "print_stump(my_decision_tree['right']['right'], my_decision_tree['right']['splitting_feature'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quiz question: What is the path of the first 3 feature splits considered along the right-most branch of my_decision_tree?\n",
    "\n",
    "Answer: term_ 36 months, grade_D"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
